{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b70ca33a-6b3a-47e4-9017-059b592d839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n"
     ]
    }
   ],
   "source": [
    "from momentfm import MOMENTPipeline\n",
    "# linear_prob\n",
    "model_lp = MOMENTPipeline.from_pretrained(\n",
    "                \"/hy-tmp/better464/MOMENT-1-large\",\n",
    "                model_kwargs={\n",
    "                    'task_name': 'forecasting',\n",
    "                    'forecast_horizon': 50,\n",
    "                    'head_dropout': 0.1,\n",
    "                    'weight_decay': 0,\n",
    "                    'freeze_encoder': True,  # Freeze the patch embedding layer\n",
    "                    'freeze_embedder': True,  # Freeze the transformer encoder\n",
    "                    'freeze_head': False,  # The linear forecasting head must be trained\n",
    "                },\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d86ea889-178f-4a34-a514-1d62d3853b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOMENTPipeline(\n",
      "  (normalizer): RevIN()\n",
      "  (tokenizer): Patching()\n",
      "  (patch_embedding): PatchEmbedding(\n",
      "    (value_embedding): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    (position_embedding): PositionalEmbedding()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (head): PretrainHead(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bcb3834d-8f11-4e6e-b2cf-b3d459461644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOMENTPipeline(\n",
      "  (normalizer): RevIN()\n",
      "  (tokenizer): Patching()\n",
      "  (patch_embedding): PatchEmbedding(\n",
      "    (value_embedding): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    (position_embedding): PositionalEmbedding()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (head): ForecastingHead(\n",
      "    (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear): Linear(in_features=65536, out_features=50, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_lp.init()\n",
    "print(model_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90a764c0-287e-4a37-8fee-ffbd3bb5450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def format_size(size):\n",
    "    # 对总参数量做格式优化\n",
    "    K, M, B = 1e3, 1e6, 1e9\n",
    "    if size == 0:\n",
    "        return '0'\n",
    "    elif size < M:\n",
    "        return f\"{size / K:.1f}K\"\n",
    "    elif size < B:\n",
    "        return f\"{size / M:.1f}M\"\n",
    "    else:\n",
    "        return f\"{size / B:.1f}B\"\n",
    "\n",
    "def get_pytorch_model_info(model: torch.nn.Module) -> (dict, list):\n",
    "    \"\"\"\n",
    "    输入一个PyTorch Model对象，返回模型的总参数量（格式化为易读格式）以及每一层的名称、尺寸、精度、参数量、是否可训练和层的类别。\n",
    "\n",
    "    :param model: PyTorch Model\n",
    "    :return: (总参数量信息, 参数列表[包括每层的名称、尺寸、数据类型、参数量、是否可训练和层的类别])\n",
    "    \"\"\"\n",
    "    params_list = []\n",
    "    total_params = 0\n",
    "    total_params_non_trainable = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        # 获取参数所属层的名称\n",
    "        layer_name = name.split('.')[0]\n",
    "        # 获取层的对象\n",
    "        layer = dict(model.named_modules())[layer_name]\n",
    "        # 获取层的类名\n",
    "        layer_class = layer.__class__.__name__\n",
    "\n",
    "        params_count = param.numel()\n",
    "        trainable = param.requires_grad\n",
    "        params_list.append({\n",
    "            'tensor': name,\n",
    "            'layer_class': layer_class,\n",
    "            'shape': str(list(param.size())),\n",
    "            'precision': str(param.dtype).split('.')[-1],\n",
    "            'params_count': str(params_count),\n",
    "            'trainable': str(trainable),\n",
    "        })\n",
    "        total_params += params_count\n",
    "        if not trainable:\n",
    "            total_params_non_trainable += params_count\n",
    "\n",
    "    total_params_trainable = total_params - total_params_non_trainable\n",
    "    \n",
    "    total_params_info = {\n",
    "        'total_params': format_size(total_params),\n",
    "        'total_params_trainable': format_size(total_params_trainable),\n",
    "        'total_params_non_trainable': format_size(total_params_non_trainable)\n",
    "    }\n",
    "\n",
    "    return total_params_info, params_list\n",
    "\n",
    "def filter_dic(it):\n",
    "    ret_list = []\n",
    "    for tup in it:\n",
    "        if tup['trainable'] == 'True':\n",
    "            ret_list.append(tup)\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75cb386c-58e8-492d-8414-f12cb7f74659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tensor': 'head.linear.weight',\n",
       "  'layer_class': 'ForecastingHead',\n",
       "  'shape': '[50, 65536]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '3276800',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'head.linear.bias',\n",
       "  'layer_class': 'ForecastingHead',\n",
       "  'shape': '[50]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '50',\n",
       "  'trainable': 'True'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_lp.init()\n",
    "# filter_dic(get_pytorch_model_info(model_lp))\n",
    "total_params_info, param_list = get_pytorch_model_info(model_lp)\n",
    "filter_dic(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77aa96c8-3482-42a0-ba81-b94f0597b553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params_info, param_list = get_pytorch_model_info(model_test)\n",
    "filter_dic(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0eff84d-fb11-4e09-bbcc-c7ec8ac46c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tensor': 'head.linear.weight',\n",
       "  'layer_class': 'ForecastingHead',\n",
       "  'shape': '[192, 65536]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '12582912',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'head.linear.bias',\n",
       "  'layer_class': 'ForecastingHead',\n",
       "  'shape': '[192]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '192',\n",
       "  'trainable': 'True'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lp.init()\n",
    "total_params_info, param_list = get_pytorch_model_info(model_lp)\n",
    "filter_dic(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35f66048-2b6c-4f6b-993c-01f94d3e2e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tensor': 'head.linear.weight',\n",
       "  'layer_class': 'ForecastingHead',\n",
       "  'shape': '[192, 65536]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '12582912',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'head.linear.bias',\n",
       "  'layer_class': 'ForecastingHead',\n",
       "  'shape': '[192]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '192',\n",
       "  'trainable': 'True'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.init()\n",
    "total_params_info, param_list = get_pytorch_model_info(model_test)\n",
    "filter_dic(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94be5b30-9380-487b-9209-ea6b6152727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型总参数量为：341248520\n",
      "当前模型可训练的参数量:8200, 占总可训练的参数量的0.002402940824475957%\n"
     ]
    }
   ],
   "source": [
    "# 模型的参数量  341248520\n",
    "# model_lp.init() \n",
    "total_params = sum(param.numel() for param in model_lp.parameters())\n",
    "print(f'模型总参数量为：{total_params}')\n",
    "# 修改bias变成可微调\n",
    "# for name, param in model_lp.named_parameters():\n",
    "#     if \"bias\" in name:  # 如果不是bias\n",
    "#         param.requires_grad = True  # 如果之前本来就不反传了\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "requires_grad_num = 0\n",
    "for name, param in model_lp.named_parameters():\n",
    "    if param.requires_grad == False:  # 不进行反传的\n",
    "        pass\n",
    "    else:  # 进行反传的\n",
    "        requires_grad_num += param.numel()\n",
    "pct_grad = requires_grad_num / total_params * 100\n",
    "print(f'当前模型可训练的参数量:{requires_grad_num}, 占总可训练的参数量的{pct_grad}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5464e96-61d9-4bca-8cdb-a450fcb69d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型总参数量为：353823424\n",
      "当前模型可训练的参数量:12583104, 占总可训练的参数量的3.5563230545188547%\n"
     ]
    }
   ],
   "source": [
    "# 模型的参数量  341248520\n",
    "model_lp.init() \n",
    "total_params = sum(param.numel() for param in model_lp.parameters())\n",
    "print(f'模型总参数量为：{total_params}')\n",
    "# 修改bias变成可微调\n",
    "# for name, param in model_lp.named_parameters():\n",
    "#     if \"bias\" in name:  # 如果不是bias\n",
    "#         param.requires_grad = True  # 如果之前本来就不反传了\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "requires_grad_num = 0\n",
    "for name, param in model_lp.named_parameters():\n",
    "    if param.requires_grad == False:  # 不进行反传的\n",
    "        pass\n",
    "    else:  # 进行反传的\n",
    "        requires_grad_num += param.numel()\n",
    "pct_grad = requires_grad_num / total_params * 100\n",
    "print(f'当前模型可训练的参数量:{requires_grad_num}, 占总可训练的参数量的{pct_grad}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a976c-b1ca-4733-b76c-03430b8ce201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 497/497 [03:11<00:00,  2.59it/s]\n",
      "100%|██████████| 169/169 [01:04<00:00,  2.63it/s]\n",
      "  0%|          | 0/497 [00:00<?, ?it/s]/usr/local/miniconda3/envs/moment/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/miniconda3/envs/moment/lib/python3.11/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 497/497 [03:24<00:00,  2.43it/s]\n",
      "100%|██████████| 169/169 [01:04<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from momentfm import MOMENTPipeline\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda.amp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from momentfm.utils.utils import control_randomness\n",
    "from momentfm.data.informer_dataset import InformerDataset\n",
    "from momentfm.utils.forecasting_metrics import get_forecasting_metrics\n",
    "\n",
    "\n",
    "\n",
    "class MOMENT_Trainer:\n",
    "    def __init__(self, seed, batch_size, epochs, forecast_horizon, mode, output_path):\n",
    "        # initialize ptbxl classification dataset\n",
    "        self.mode = mode\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.output_path = output_path\n",
    "        self.train_dataset = InformerDataset(data_split=\"train\", random_seed=seed, forecast_horizon=self.forecast_horizon)\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.test_dataset = InformerDataset(data_split=\"test\", random_seed=seed, forecast_horizon=self.forecast_horizon)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "         #create log file to store training logs \n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "        self.log_file = open(os.path.join(self.output_path, f'log_{self.mode}.txt'), 'w')\n",
    "        sys.stdout = self.log_file\n",
    "        # linear probing: only train classification head\n",
    "        # finetuning: train both encoder and classification head\n",
    "        # unsupervised learning: train SVM on top of MOMENT embeddings\n",
    "        if self.mode == 'linear_probing':\n",
    "            self.model = MOMENTPipeline.from_pretrained(\n",
    "                \"/hy-tmp/better464/MOMENT-1-large\",\n",
    "                model_kwargs={\n",
    "                    'task_name': 'forecasting',\n",
    "                    'forecast_horizon': self.forecast_horizon,\n",
    "                    'head_dropout': 0.1,\n",
    "                    'weight_decay': 0,\n",
    "                    'freeze_encoder': True,  # Freeze the patch embedding layer\n",
    "                    'freeze_embedder': True,  # Freeze the transformer encoder\n",
    "                    'freeze_head': False,  # The linear forecasting head must be trained\n",
    "                },\n",
    "            )\n",
    "\n",
    "        self.model.init()\n",
    "        print('Model initialized, training mode: ', self.mode)\n",
    "        #using cross MSE loss for forecasting\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "        if self.mode == 'linear_probing':\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "            # Create a OneCycleLR scheduler\n",
    "            max_lr = 1e-4\n",
    "            total_steps = len(self.train_loader) * self.epochs\n",
    "            self.scheduler = OneCycleLR(self.optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.3)\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch+1}/{self.epochs}')\n",
    "            # self.log_file.write(f'Epoch {epoch+1}/{self.epochs}\\n')\n",
    "            self.epoch = epoch + 1\n",
    "\n",
    "            if self.mode == 'linear_probing':\n",
    "                self.train_epoch_lp()\n",
    "                self.evaluate_epoch()\n",
    "            else:\n",
    "                raise ValueError('Invalid mode, please choose linear_probing, full_finetuning, or unsupervised_representation_learning')\n",
    "\n",
    "                \n",
    "    ####################################Training function##################################\n",
    "    def train_epoch_lp(self):\n",
    "        '''\n",
    "        Train only forecasting head-linear_probing\n",
    "        '''\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Move the model to the GPU\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # Move the loss function to the GPU\n",
    "        self.criterion = self.criterion.to(self.device)\n",
    "        \n",
    "        # Enable mixed precision training\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        # Gradient clipping value\n",
    "        max_norm = 5.0\n",
    "        \n",
    "        losses = []\n",
    "        for timeseries, forecast, input_mask in tqdm(self.train_loader, total=len(self.train_loader)):\n",
    "            # Move the data to the GPU\n",
    "            timeseries = timeseries.float().to(self.device)\n",
    "            input_mask = input_mask.to(self.device)\n",
    "            forecast = forecast.float().to(self.device)\n",
    "    \n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = self.model(timeseries, input_mask)\n",
    "                \n",
    "            loss = self.criterion(output.forecast, forecast)\n",
    "    \n",
    "            # Scales the loss for mixed precision training\n",
    "            scaler.scale(loss).backward()\n",
    "    \n",
    "            # Clip gradients\n",
    "            scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm)\n",
    "    \n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "            losses.append(loss.item())\n",
    "    \n",
    "        losses = np.array(losses)\n",
    "        average_loss = np.average(losses)\n",
    "        print(f\"Train loss: {average_loss:.3f}\")\n",
    "        # Step the learning rate scheduler\n",
    "        self.scheduler.step()\n",
    "  \n",
    "\n",
    "    ####################################Evaluate function##################################\n",
    "    def evaluate_epoch(self):        \n",
    "        # Evaluate the model on the test split\n",
    "        trues, preds, histories, losses = [], [], [], []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for timeseries, forecast, input_mask in tqdm(self.test_loader, total=len(self.test_loader)):\n",
    "            # Move the data to the GPU\n",
    "                timeseries = timeseries.float().to(self.device)\n",
    "                input_mask = input_mask.to(self.device)\n",
    "                forecast = forecast.float().to(self.device)\n",
    "    \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = self.model(timeseries, input_mask)\n",
    "                \n",
    "                loss = self.criterion(output.forecast, forecast)                \n",
    "                losses.append(loss.item())\n",
    "    \n",
    "                trues.append(forecast.detach().cpu().numpy())\n",
    "                preds.append(output.forecast.detach().cpu().numpy())\n",
    "                histories.append(timeseries.detach().cpu().numpy())\n",
    "        \n",
    "        losses = np.array(losses)\n",
    "        average_loss = np.average(losses)\n",
    "        self.model.train()\n",
    "    \n",
    "        trues = np.concatenate(trues, axis=0)\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        histories = np.concatenate(histories, axis=0)\n",
    "        \n",
    "        metrics = get_forecasting_metrics(y=trues, y_hat=preds, reduction='mean')\n",
    "    \n",
    "        print(f\"Test Loss: {average_loss:.3f}| Test MSE: {metrics.mse:.3f} | Test MAE: {metrics.mae:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seed = 13\n",
    "    control_randomness(seed)\n",
    "    batch_size = 16\n",
    "    epochs = 2\n",
    "    forecast_horizon = 192\n",
    "    output_path = '/root/moment/tuning_exp/logs'\n",
    "    mode = 'linear_probing'\n",
    "    trainer = MOMENT_Trainer(seed, batch_size, epochs, forecast_horizon, mode, output_path)\n",
    "    trainer.train()\n",
    "    trainer.log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d0f8d04-08bd-4851-929c-fd86b3ad507f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n",
      "Model initialized, training mode:  linear_probing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 993/993 [02:55<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:57<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test MSE: 0.421 | Test MAE: 0.431\n"
     ]
    }
   ],
   "source": [
    "from momentfm import MOMENTPipeline\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda.amp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from momentfm.utils.utils import control_randomness\n",
    "from momentfm.data.informer_dataset import InformerDataset\n",
    "from momentfm.utils.forecasting_metrics import get_forecasting_metrics\n",
    "\n",
    "\n",
    "\n",
    "class MOMENT_Trainer:\n",
    "    def __init__(self, seed, batch_size, forecast_horizon, mode):\n",
    "        # initialize ptbxl classification dataset\n",
    "        self.mode = mode\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.batch_size = batch_size\n",
    "        train_dataset = InformerDataset(data_split=\"train\", random_seed=seed, forecast_horizon=self.forecast_horizon)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        test_dataset = InformerDataset(data_split=\"test\", random_seed=seed, forecast_horizon=self.forecast_horizon)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "        # linear probing: only train classification head\n",
    "        # finetuning: train both encoder and classification head\n",
    "        # unsupervised learning: train SVM on top of MOMENT embeddings\n",
    "        if self.mode == 'linear_probing':\n",
    "            self.model = MOMENTPipeline.from_pretrained(\n",
    "                \"/hy-tmp/better464/MOMENT-1-large\",\n",
    "                model_kwargs={\n",
    "                    'task_name': 'forecasting',\n",
    "                    'forecast_horizon': 192,\n",
    "                    'head_dropout': 0.1,\n",
    "                    'weight_decay': 0,\n",
    "                    'freeze_encoder': True,  # Freeze the patch embedding layer\n",
    "                    'freeze_embedder': True,  # Freeze the transformer encoder\n",
    "                    'freeze_head': False,  # The linear forecasting head must be trained\n",
    "                },\n",
    "            )\n",
    "\n",
    "        self.model.init()\n",
    "        print('Model initialized, training mode: ', self.mode)\n",
    "        #using cross MSE loss for forecasting\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        if self.mode == 'linear_probing':\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        cur_epoch = 0\n",
    "        max_epoch = 1\n",
    "\n",
    "        # Move the model to the GPU\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "        # Move the loss function to the GPU\n",
    "        criterion = criterion.to(device)\n",
    "        \n",
    "        # Enable mixed precision training\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        # Create a OneCycleLR scheduler\n",
    "        max_lr = 1e-4\n",
    "        total_steps = len(train_loader) * max_epoch\n",
    "        scheduler = OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.3)\n",
    "        \n",
    "        # Gradient clipping value\n",
    "        max_norm = 5.0\n",
    "        \n",
    "        while cur_epoch < max_epoch:\n",
    "            losses = []\n",
    "            for timeseries, forecast, input_mask in tqdm(train_loader, total=len(train_loader)):\n",
    "                # Move the data to the GPU\n",
    "                timeseries = timeseries.float().to(device)\n",
    "                input_mask = input_mask.to(device)\n",
    "                forecast = forecast.float().to(device)\n",
    "        \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = self.model(timeseries, input_mask)\n",
    "                \n",
    "                loss = criterion(output.forecast, forecast)\n",
    "        \n",
    "                # Scales the loss for mixed precision training\n",
    "                scaler.scale(loss).backward()\n",
    "        \n",
    "                # Clip gradients\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm)\n",
    "        \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "                losses.append(loss.item())\n",
    "        \n",
    "            losses = np.array(losses)\n",
    "            average_loss = np.average(losses)\n",
    "            print(f\"Epoch {cur_epoch}: Train loss: {average_loss:.3f}\")\n",
    "        \n",
    "            # Step the learning rate scheduler\n",
    "            scheduler.step()\n",
    "            cur_epoch += 1\n",
    "            \n",
    "            # Evaluate the model on the test split\n",
    "            trues, preds, histories, losses = [], [], [], []\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for timeseries, forecast, input_mask in tqdm(test_loader, total=len(test_loader)):\n",
    "                # Move the data to the GPU\n",
    "                    timeseries = timeseries.float().to(device)\n",
    "                    input_mask = input_mask.to(device)\n",
    "                    forecast = forecast.float().to(device)\n",
    "        \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        output = self.model(timeseries, input_mask)\n",
    "                    \n",
    "                    loss = criterion(output.forecast, forecast)                \n",
    "                    losses.append(loss.item())\n",
    "        \n",
    "                    trues.append(forecast.detach().cpu().numpy())\n",
    "                    preds.append(output.forecast.detach().cpu().numpy())\n",
    "                    histories.append(timeseries.detach().cpu().numpy())\n",
    "            \n",
    "            losses = np.array(losses)\n",
    "            average_loss = np.average(losses)\n",
    "            self.model.train()\n",
    "        \n",
    "            trues = np.concatenate(trues, axis=0)\n",
    "            preds = np.concatenate(preds, axis=0)\n",
    "            histories = np.concatenate(histories, axis=0)\n",
    "            \n",
    "            metrics = get_forecasting_metrics(y=trues, y_hat=preds, reduction='mean')\n",
    "        \n",
    "            print(f\"Epoch {cur_epoch}: Test MSE: {metrics.mse:.3f} | Test MAE: {metrics.mae:.3f}\")\n",
    "        \n",
    "        # 模型的参数量  341248520\n",
    "        # total_params = sum(param.numel() for param in  self.model.parameters())\n",
    "        # print(f'模型总参数量为：{total_params}')\n",
    "\n",
    "        # # 修改bias变成可微调\n",
    "        # for name, param in self.model.named_parameters():\n",
    "        #     if \"bias\" in name:  # 如果不是bias\n",
    "        #         param.requires_grad = True  # 如果之前本来就不反传了\n",
    "        #     else:\n",
    "        #         pass\n",
    "\n",
    "        # requires_grad_num = 0\n",
    "        # for name, param in self.model.named_parameters():\n",
    "        #     if param.requires_grad == False:  # 不进行反传的\n",
    "        #         pass\n",
    "        #     else:  # 进行反传的\n",
    "        #         requires_grad_num += param.numel()\n",
    "        # pct_grad = requires_grad_num / total_params * 100\n",
    "        # print(f'当前模型可训练的参数量:{requires_grad_num}, 占总可训练的参数量的{pct_grad}%')\n",
    "\n",
    "        \n",
    "        # print(get_pytorch_model_info(self.model))\n",
    "if __name__ == '__main__':\n",
    "    seed = 13\n",
    "    control_randomness(seed)\n",
    "    batch_size = 8\n",
    "    forecast_horizon = 192\n",
    "    mode = 'linear_probing'\n",
    "    MOMENT_Trainer(seed, batch_size, forecast_horizon, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78f5f518-4a5b-4333-872c-a3b612266b81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trues' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming histories, trues, and preds are your lists containing the data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Extracting the first data point\u001b[39;00m\n\u001b[1;32m      6\u001b[0m channel_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m7\u001b[39m) \u001b[38;5;66;03m# There are 7 channels in this dataset\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m time_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, trues\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \n\u001b[1;32m      9\u001b[0m history \u001b[38;5;241m=\u001b[39m histories[time_index, channel_idx, :] \n\u001b[1;32m     10\u001b[0m true \u001b[38;5;241m=\u001b[39m trues[time_index, channel_idx, :]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trues' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming histories, trues, and preds are your lists containing the data\n",
    "# Extracting the first data point\n",
    "\n",
    "channel_idx = np.random.randint(0, 7) # There are 7 channels in this dataset\n",
    "time_index = np.random.randint(0, trues.shape[0]) \n",
    "\n",
    "history = histories[time_index, channel_idx, :] \n",
    "true = trues[time_index, channel_idx, :]\n",
    "pred = preds[time_index, channel_idx, :]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plotting the first time series from history\n",
    "plt.plot(range(len(history)), history, label='History (512 timesteps)', c='darkblue')\n",
    "\n",
    "# Plotting ground truth and prediction\n",
    "num_forecasts = len(true)\n",
    "\n",
    "offset = len(history)\n",
    "plt.plot(range(offset, offset + len(true)), true, label='Ground Truth (192 timesteps)', color='darkblue', linestyle='--', alpha=0.5)\n",
    "plt.plot(range(offset, offset + len(pred)), pred, label='Forecast (192 timesteps)', color='red', linestyle='--')\n",
    "\n",
    "plt.title(f\"ETTh1 (Hourly) -- (idx={time_index}, channel={channel_idx})\", fontsize=18)\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d36476-e216-4d82-ab9f-c7a46f3697a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moment",
   "language": "python",
   "name": "moment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
