{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef43498-7489-40e9-8adc-51ded6eb9a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hy-tmp/sz-moment/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/hy-tmp/sz-moment/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n",
      "模型总参数量为：341248520\n",
      "当前模型可训练的参数量:8200, 占总可训练的参数量的0.002402940824475957%\n",
      "LoRA enabled\n",
      "trainable params: 6,291,456 || all params: 347,539,976 || trainable%: 1.810282682415792\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from momentfm import MOMENTPipeline\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "                                        r=64,\n",
    "                                        lora_alpha=32,\n",
    "                                        target_modules=[\"q\", \"v\"],\n",
    "                                        lora_dropout=0.05,\n",
    "                                        )\n",
    "\n",
    "model_base = MOMENTPipeline.from_pretrained(\n",
    "                \"/hy-tmp/better464/MOMENT-1-large\", #hy-tmp/better464/MOMENT-1-large\n",
    "                model_kwargs={\n",
    "                    'task_name': 'forecasting',\n",
    "                    'forecast_horizon': 20,\n",
    "                    'head_dropout': 0.1,\n",
    "                    'weight_decay': 0,\n",
    "                    'freeze_encoder': True,  # Freeze the patch embedding layer\n",
    "                    'freeze_embedder': True,  # Freeze the transformer encoder\n",
    "                    'freeze_head': False,  # The linear forecasting head must be trained\n",
    "                },\n",
    "                #use_safetensors = False\n",
    "            )\n",
    "# 观察当前模型的参数量\n",
    "total_params = sum(param.numel() for param in model_base.parameters())\n",
    "print(f'模型总参数量为：{total_params}')\n",
    "requires_grad_num = 0\n",
    "for name, param in model_base.named_parameters():\n",
    "    if param.requires_grad == False:  # 不进行反传的\n",
    "        pass\n",
    "    else:  # 进行反传的\n",
    "        requires_grad_num += param.numel()\n",
    "pct_grad = requires_grad_num / total_params * 100\n",
    "print(f'当前模型可训练的参数量:{requires_grad_num}, 占总可训练的参数量的{pct_grad}%')\n",
    "\n",
    "# LoRA初始化\n",
    "model = get_peft_model(model_base, lora_config)\n",
    "print('LoRA enabled')\n",
    "model.print_trainable_parameters()\n",
    "# 'trainable params: 6,291,456 || all params: 347,539,976 || trainable%: 1.810282682415792'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "733506d3-b021-42b0-be3e-6e7405ec85a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MOMENTPipeline(\n",
      "      (normalizer): RevIN()\n",
      "      (tokenizer): Patching()\n",
      "      (patch_embedding): PatchEmbedding(\n",
      "        (value_embedding): Linear(in_features=8, out_features=1024, bias=False)\n",
      "        (position_embedding): PositionalEmbedding()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 1024)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 16)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "                  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-23): 23 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "                  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (head): PretrainHead(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear): Linear(in_features=1024, out_features=8, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1030c3-8499-44da-9602-b08408ce4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def format_size(size):\n",
    "    # 对总参数量做格式优化\n",
    "    K, M, B = 1e3, 1e6, 1e9\n",
    "    if size == 0:\n",
    "        return '0'\n",
    "    elif size < M:\n",
    "        return f\"{size / K:.1f}K\"\n",
    "    elif size < B:\n",
    "        return f\"{size / M:.1f}M\"\n",
    "    else:\n",
    "        return f\"{size / B:.1f}B\"\n",
    "\n",
    "def get_pytorch_model_info(model: torch.nn.Module) -> (dict, list):\n",
    "    \"\"\"\n",
    "    输入一个PyTorch Model对象，返回模型的总参数量（格式化为易读格式）以及每一层的名称、尺寸、精度、参数量、是否可训练和层的类别。\n",
    "\n",
    "    :param model: PyTorch Model\n",
    "    :return: (总参数量信息, 参数列表[包括每层的名称、尺寸、数据类型、参数量、是否可训练和层的类别])\n",
    "    \"\"\"\n",
    "    params_list = []\n",
    "    total_params = 0\n",
    "    total_params_non_trainable = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        # 获取参数所属层的名称\n",
    "        layer_name = name.split('.')[0]\n",
    "        # 获取层的对象\n",
    "        layer = dict(model.named_modules())[layer_name]\n",
    "        # 获取层的类名\n",
    "        layer_class = layer.__class__.__name__\n",
    "\n",
    "        params_count = param.numel()\n",
    "        trainable = param.requires_grad\n",
    "        params_list.append({\n",
    "            'tensor': name,\n",
    "            'layer_class': layer_class,\n",
    "            'shape': str(list(param.size())),\n",
    "            'precision': str(param.dtype).split('.')[-1],\n",
    "            'params_count': str(params_count),\n",
    "            'trainable': str(trainable),\n",
    "        })\n",
    "        total_params += params_count\n",
    "        if not trainable:\n",
    "            total_params_non_trainable += params_count\n",
    "\n",
    "    total_params_trainable = total_params - total_params_non_trainable\n",
    "    \n",
    "    total_params_info = {\n",
    "        'total_params': format_size(total_params),\n",
    "        'total_params_trainable': format_size(total_params_trainable),\n",
    "        'total_params_non_trainable': format_size(total_params_non_trainable)\n",
    "    }\n",
    "\n",
    "    return total_params_info, params_list\n",
    "\n",
    "def filter_dic(it):\n",
    "    ret_list = []\n",
    "    for tup in it:\n",
    "        if tup['trainable'] == 'True':\n",
    "            ret_list.append(tup)\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b112f28-c39b-4e1b-834b-28c09d255564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tensor': 'base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_A.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[64, 1024]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'},\n",
       " {'tensor': 'base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_B.default.weight',\n",
       "  'layer_class': 'LoraModel',\n",
       "  'shape': '[1024, 64]',\n",
       "  'precision': 'float32',\n",
       "  'params_count': '65536',\n",
       "  'trainable': 'True'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params_info, param_list = get_pytorch_model_info(model)\n",
    "filter_dic(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c310e37f-f613-43d3-b65f-78ed5c37c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class InformerDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        forecast_horizon = 192,\n",
    "        data_split = \"train\",\n",
    "        data_stride_len = 1,\n",
    "        task_name = \"forecasting\",\n",
    "        random_seed = 42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        forecast_horizon : int\n",
    "            Length of the prediction sequence.\n",
    "        data_split : str\n",
    "            Split of the dataset, 'train' or 'test'.\n",
    "        data_stride_len : int\n",
    "            Stride length when generating consecutive\n",
    "            time series windows.\n",
    "        task_name : str\n",
    "            The task that the dataset is used for. One of\n",
    "            'forecasting', or  'imputation'.\n",
    "        random_seed : int\n",
    "            Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        self.seq_len = 512\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.full_file_path_and_name = \"../../data/ETTh1.csv\"\n",
    "        self.data_split = data_split\n",
    "        self.data_stride_len = data_stride_len\n",
    "        self.task_name = task_name\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Read data\n",
    "        self._read_data()\n",
    "\n",
    "    def _get_borders(self):\n",
    "        n_train = 12 * 30 * 24\n",
    "        n_val = 4 * 30 * 24\n",
    "        n_test = 4 * 30 * 24\n",
    "\n",
    "        train_end = n_train\n",
    "        val_end = n_train + n_val\n",
    "        test_start = val_end - self.seq_len\n",
    "        test_end = test_start + n_test + self.seq_len\n",
    "\n",
    "        train = slice(0, train_end)\n",
    "        test = slice(test_start, test_end)\n",
    "\n",
    "        return train, test\n",
    "\n",
    "    def _read_data(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df = pd.read_csv(self.full_file_path_and_name)\n",
    "        self.length_timeseries_original = df.shape[0]\n",
    "        self.n_channels = df.shape[1] - 1\n",
    "\n",
    "        df.drop(columns=[\"date\"], inplace=True)\n",
    "        df = df.infer_objects(copy=False).interpolate(method=\"cubic\")\n",
    "\n",
    "        data_splits = self._get_borders()\n",
    "\n",
    "        train_data = df[data_splits[0]]\n",
    "        self.scaler.fit(train_data.values)\n",
    "        df = self.scaler.transform(df.values)\n",
    "\n",
    "        if self.data_split == \"train\":\n",
    "            self.data = df[data_splits[0], :]\n",
    "        elif self.data_split == \"test\":\n",
    "            self.data = df[data_splits[1], :]\n",
    "\n",
    "        self.length_timeseries = self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq_start = self.data_stride_len * index\n",
    "        seq_end = seq_start + self.seq_len\n",
    "        input_mask = np.ones(self.seq_len)\n",
    "\n",
    "        if self.task_name == \"forecasting\":\n",
    "            pred_end = seq_end + self.forecast_horizon\n",
    "\n",
    "            if pred_end > self.length_timeseries:\n",
    "                pred_end = self.length_timeseries\n",
    "                seq_end = seq_end - self.forecast_horizon\n",
    "                seq_start = seq_end - self.seq_len\n",
    "\n",
    "            timeseries = self.data[seq_start:seq_end, :].T\n",
    "            forecast = self.data[seq_end:pred_end, :].T\n",
    "\n",
    "            return timeseries, forecast, input_mask\n",
    "\n",
    "        elif self.task_name == \"imputation\":\n",
    "            if seq_end > self.length_timeseries:\n",
    "                seq_end = self.length_timeseries\n",
    "                seq_end = seq_end - self.seq_len\n",
    "\n",
    "            timeseries = self.data[seq_start:seq_end, :].T\n",
    "\n",
    "            return timeseries, input_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.task_name == \"imputation\":\n",
    "            return (self.length_timeseries - self.seq_len) // self.data_stride_len + 1\n",
    "        elif self.task_name == \"forecasting\":\n",
    "            return (\n",
    "                self.length_timeseries - self.seq_len - self.forecast_horizon\n",
    "            ) // self.data_stride_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3341401-0e15-4e27-900c-229c1e0165a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda.amp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "from momentfm.utils.utils import control_randomness\n",
    "from momentfm.utils.forecasting_metrics import get_forecasting_metrics\n",
    "\n",
    "# Set random seeds for PyTorch, Numpy etc.\n",
    "control_randomness(seed=13) \n",
    "\n",
    "# Load data\n",
    "train_dataset = InformerDataset(data_split=\"train\", random_seed=13, forecast_horizon=192)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = InformerDataset(data_split=\"test\", random_seed=13, forecast_horizon=192)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cur_epoch = 0\n",
    "max_epoch = 1\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Move the loss function to the GPU\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Enable mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Create a OneCycleLR scheduler\n",
    "max_lr = 1e-4\n",
    "total_steps = len(train_loader) * max_epoch\n",
    "scheduler = OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18b98b-0aed-4cfc-9c65-257dac444bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sz-moment",
   "language": "python",
   "name": "sz-moment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
